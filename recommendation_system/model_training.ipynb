{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XCiokfzGoJBT"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import cv2\n",
    "import pickle\n",
    "from natsort import natsorted\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import tensorflow as tf\n",
    "import os\n",
    "from tensorflow import keras\n",
    "from collections import deque\n",
    "import random"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "z0kxFTnMt231"
   },
   "source": [
    "## Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qj-XlVXKt5z3"
   },
   "outputs": [],
   "source": [
    "os.chdir('./datasets')\n",
    "with open('replay_buffer.pkl', 'wb') as f:\n",
    "    replay_buffer = pickle.load(f) # deserialize the list"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "39Nb_QzeWdNQ"
   },
   "source": [
    "## Paramters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vZ6hROvQWfcT"
   },
   "outputs": [],
   "source": [
    "replay_buffer = deque(maxlen=50)\n",
    "\n",
    "epsilon = 1.0\n",
    "epsilon_min = 0.1\n",
    "epsilon_decay = 0.995\n",
    "\n",
    "episodes = 1000"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "9lizlHzts5dI"
   },
   "source": [
    "## Model Creation and Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "k11sDunkohre"
   },
   "outputs": [],
   "source": [
    "def simple_Q_model_creation(layers=[], learning_rate = 0.001):\n",
    "\n",
    "    '''\n",
    "    The input will be a np.array of length 50, with each index representing a tag\n",
    "    The value of each index is the amount of saves that tag has received\n",
    "\n",
    "    The output will be a np.array of length 50, with each index representing a tag\n",
    "    The value of each index is either 1 or 0 indicating whether the tag should be recommended or not\n",
    "    '''\n",
    "\n",
    "    model = tf.keras.Sequential()\n",
    "    input_layer = tf.keras.Input(shape=(50,))\n",
    "    for layer in layers:\n",
    "        model.add(tf.keras.layers.Dense(layer, activation='relu'))\n",
    "        model.add(tf.keras.layers.Dropout(0.2))\n",
    "    model.add(tf.keras.layers.Dense(50, activation='linear'))\n",
    "\n",
    "    opt = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "\n",
    "    model.compile(optimizer=opt, loss='mse', metrics=['accuracy'])\n",
    "\n",
    "    model.summary()\n",
    "\n",
    "    return model\n",
    "\n",
    "q_network = simple_Q_model_creation([256, 128, 64])\n",
    "target_network = simple_Q_model_creation([256, 128, 64])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-3HUYSTotkF7"
   },
   "outputs": [],
   "source": [
    "\n",
    "def store_experience(state, action, reward, next_state, done):\n",
    "    replay_buffer.append((state, action, reward, next_state, done))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AoENSLiHtp6F"
   },
   "outputs": [],
   "source": [
    "\n",
    "def select_action(state, epsilon):\n",
    "    if np.random.rand() < epsilon:\n",
    "        return np.random.randint(0, 2, size=50)\n",
    "    q_values = q_network.predict(state.reshape(1, -1))\n",
    "    return (q_values[0] > 0).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HlIKnEQ3tyKT"
   },
   "outputs": [],
   "source": [
    "losses = []\n",
    "\n",
    "def replay(batch_size=32, gamma=0.99):\n",
    "    if len(replay_buffer) < batch_size:\n",
    "        return\n",
    "\n",
    "    minibatch = random.sample(replay_buffer, batch_size)\n",
    "    for state, action, reward, next_state, done in minibatch:\n",
    "        target = reward\n",
    "        if not done:\n",
    "            target += gamma * np.amax(target_network.predict(next_state.reshape(1, -1))[0])\n",
    "\n",
    "        target_f = q_network.predict(state.reshape(1, -1))\n",
    "        target_f[0][np.argmax(action)] = target\n",
    "\n",
    "        history = q_network.fit(state.reshape(1, -1), target_f, epochs=1)\n",
    "        losses.append(history.history['loss'][0])\n",
    "\n",
    "\n",
    "    global epsilon\n",
    "    if epsilon > epsilon_min:\n",
    "        epsilon *= epsilon_decay\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Y-fqplqHt0ve"
   },
   "outputs": [],
   "source": [
    "# Replace with actual data once dataset is generated\n",
    "\n",
    "# for episode in range(episodes):\n",
    "#     state = np.random.rand(50)\n",
    "#     done = False\n",
    "\n",
    "#     while not done:\n",
    "\n",
    "#         action = select_action(state, epsilon)\n",
    "\n",
    "#         next_state = np.random.rand(50)\n",
    "#         reward = np.sum(action)\n",
    "#         done = np.random.rand() > 0.95\n",
    "\n",
    "#         store_experience(state, action, reward, next_state, done)\n",
    "#         state = next_state\n",
    "\n",
    "#         replay()\n",
    "\n",
    "\n",
    "#     if episode % 10 == 0:\n",
    "#         target_network.set_weights(q_network.get_weights())\n",
    "\n",
    "#     print(f\"Episode {episode + 1}/{episodes}, Epsilon: {epsilon:.2f}\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "u2hKgUrcs-0c"
   },
   "source": [
    "## Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "itJIZPUBtA2Q"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vG-5XJ2YtKAp"
   },
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "TxjbLDL7tBVf"
   },
   "source": [
    "## Explainability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KL4xn7X5t_hm"
   },
   "outputs": [],
   "source": [
    "state = np.random.rand(50)  # Replace with actual state\n",
    "q_values = q_network.predict(state.reshape(1, -1))\n",
    "recommendations = (q_values[0] > 0).astype(int)\n",
    "print(\"Recommendations:\", recommendations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "g87CXM14tDqE"
   },
   "outputs": [],
   "source": [
    "plt.plot(losses)\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training Progress: Loss')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
